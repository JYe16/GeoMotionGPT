import torch

def inspect_model_gradients(model, motion_ids_tensor):
    """
    å…¨é¢æ£€æŸ¥æ¨¡å‹çš„è®­ç»ƒçŠ¶æ€ï¼š
    1. æ‰“å° requires_grad ä¸º True çš„å‚æ•°ã€‚
    2. è¿è¡Œä¸€æ¬¡ Dummy Backwardï¼ŒéªŒè¯æ¢¯åº¦æ˜¯å¦çœŸçš„åªæµå‘äº† Motion Tokenã€‚
    """
    print("\n" + "=" * 50)
    print("ğŸ” æ¨¡å‹è§£å†»çŠ¶æ€æ·±åº¦æ£€æŸ¥ (Deep Inspection)")
    print("=" * 50)

    # --- 1. é™æ€æ£€æŸ¥: å“ªäº›å‚æ•°è¢«æ ‡è®°ä¸ºå¯è®­ç»ƒ ---
    print("\n[1. Static Check] List of Parameters with requires_grad=True:")
    trainable_count = 0
    total_count = 0

    for name, param in model.named_parameters():
        total_count += param.numel()
        if param.requires_grad:
            print(f"  âœ… Trainable: {name:<60} | Shape: {list(param.shape)}")
            trainable_count += param.numel()

    ratio = trainable_count / total_count * 100
    print(f"\n  ğŸ“Š ç»Ÿè®¡: {trainable_count:,} / {total_count:,} params are trainable ({ratio:.2f}%)")
    print("  âš ï¸  æ³¨æ„ï¼šå¯¹äº Embeddings å’Œ LM Headï¼Œæ˜¾ç¤º 'Trainable' æ˜¯æ­£å¸¸çš„ã€‚")
    print("      å¿…é¡»é€šè¿‡ä¸‹é¢çš„åŠ¨æ€æ£€æŸ¥æ¥ç¡®è®¤ Hook æ˜¯å¦ç”Ÿæ•ˆã€‚")

    # --- 2. åŠ¨æ€æ£€æŸ¥: æ¢¯åº¦æ˜¯å¦çœŸçš„è¢« Hook è¿‡æ»¤äº† ---
    print("\n[2. Dynamic Check] Running Dummy Backward to verify Gradient Hooks...")

    device = model.device
    emb_layer = model.get_input_embeddings()
    lm_head = model.lm_head

    # æ„é€ ä¸€ä¸ªå‡è¾“å…¥ï¼š[æ™®é€šToken, MotionToken]
    # æ‰¾ä¸€ä¸ªè‚¯å®šä¸æ˜¯ Motion çš„æ™®é€š Token ID (æ¯”å¦‚ 100)
    normal_token_id = 100
    while normal_token_id in motion_ids_tensor:
        normal_token_id += 1

    motion_token_id = motion_ids_tensor[0].item()  # å–ç¬¬ä¸€ä¸ªåŠ¨ä½œ Token

    print(f"  ğŸ§ª Test Input: [Normal ID: {normal_token_id}] vs [Motion ID: {motion_token_id}]")

    dummy_input = torch.tensor([[motion_token_id, normal_token_id]], device=device)
    dummy_labels = dummy_input.clone()

    # æ¸…ç©ºæ¢¯åº¦
    model.zero_grad()

    # å‰å‘ + åå‘
    outputs = model(input_ids=dummy_input, labels=dummy_labels)
    loss = outputs.loss
    loss.backward()

    # --- éªŒè¯ Input Embeddings ---
    print(f"\n  ğŸ” Checking Input Embeddings (layer: {type(emb_layer).__name__}):")
    if emb_layer.weight.grad is None:
        print("  âŒ CRITICAL ERROR: Embedding layer has NO gradient at all! (Check if inputs require grad)")
    else:
        grad = emb_layer.weight.grad
        normal_grad_norm = grad[normal_token_id].norm().item()
        motion_grad_norm = grad[motion_token_id].norm().item()

        print(f"    - Normal Token Grad Norm: {normal_grad_norm:.6f} (Expecting 0.0)")
        print(f"    - Motion Token Grad Norm: {motion_grad_norm:.6f} (Expecting > 0.0)")

        if normal_grad_norm == 0.0 and motion_grad_norm > 0.0:
            print("    âœ… Embedding Hook is WORKING! (Text frozen, Motion training)")
        elif normal_grad_norm > 0.0:
            print("    âŒ FAILURE: Normal tokens are receiving gradients! Hook NOT working.")
        else:
            print("    â“ WARNING: Motion tokens have 0 gradient. Maybe bad initialization or loss is 0?")

    # --- éªŒè¯ LM Head (Output) ---
    if lm_head.weight is emb_layer.weight:
        print("\n  â„¹ï¸  LM Head is TIED to Input Embeddings. (Skipping separate check)")
    else:
        print(f"\n  ğŸ” Checking LM Head (layer: {type(lm_head).__name__}):")
        if lm_head.weight.grad is None:
            print("  âŒ CRITICAL ERROR: LM Head has NO gradient!")
        else:
            grad = lm_head.weight.grad
            normal_grad_norm = grad[normal_token_id].norm().item()
            motion_grad_norm = grad[motion_token_id].norm().item()

            print(f"    - Normal Token Grad Norm: {normal_grad_norm:.6f} (Expecting 0.0)")
            print(f"    - Motion Token Grad Norm: {motion_grad_norm:.6f} (Expecting > 0.0)")

            if normal_grad_norm == 0.0 and motion_grad_norm > 0.0:
                print("    âœ… LM Head Hook is WORKING!")
            else:
                print("    âŒ FAILURE on LM Head.")

    # æ¸…ç†ç”¨äºæµ‹è¯•çš„æ¢¯åº¦ï¼Œä»¥å…å½±å“åç»­è®­ç»ƒ
    model.zero_grad()
    print("\n" + "=" * 50 + "\n")